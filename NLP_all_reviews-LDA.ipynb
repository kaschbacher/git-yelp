{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gensim\n",
    "from gensim import corpora, models, similarities, utils\n",
    "from pprint import pprint   # pretty-printer\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymysql\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "import KAsql2 as ka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE ortho;\n"
     ]
    }
   ],
   "source": [
    "# from KAsql import query_SQL\n",
    "sql = 'SELECT id,business_id,stars,comment FROM review;' \n",
    "rows = ka.query_SQL(sql)#returns a tuple within a tuple: each row is a tuple, inside one big tuple.\n",
    "\n",
    "documents = []#type list\n",
    "ndocs = np.shape(rows)[0]\n",
    "\n",
    "#reviews = [[review for review in rows[3]] for row in rows]\n",
    "for row in rows:\n",
    "    documents.append(row[3])# row[3] is type string, documents is type list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some background: I rolled my ankle and had a really nasty sprain on Sunday. I tried to R.I.C.E. but had this nagging feeling that I should get it checked out. I had gone with a friend's recommendation with a doc but she was on vacation for 2 weeks and her assistant couldn't offer an honest recommendation to another doctor. Sooo here's how Yelp comes into play. Dr Chen has great rates so why not. His friendly staff sets me up with an appointment same day after I explain my situation. I show up and this is definitely a busy office. For people who complain about waiting past their appointment time: Well... don't see a populargood doctor. There's a reason everyone is waiting. I use to work in doctor offices and they do try their best to stay on time but emergencies come up. Okay, moving on. There were some unknowns about my insurance plan but guess what: you pay for medical care. I care that my doctor is competent, has good bedside manners, and isn't recommending outrageous surgeries. Dr. Chen is warm, funny, and made sure that I knew what I was getting into and how to take care of it. Granted my insurance is out of network with Dr. Chen. I'd pay a nominal fee to see him. I think that says a lot. Their fee is also super fair. Insurance is tough but Dr. Chen's office has been really great to work with. They definitely go above and beyond to make sure you understand your coverage and the possible out of pocket cost.\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])# prints one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def canonicalize(word):    \n",
    "    bads = ['dr.','dr',\"n't\",\"'ve\",\"'re\",'wo',\"'m\"]\n",
    "    goods = ['doctor','doctor','not','have','are','will','am']\n",
    "\n",
    "    try:\n",
    "        idx = bads.index(word)\n",
    "        word=goods[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unify_pronouns(word):\n",
    "    \"\"\" map many onto one for pronouns - parts of speech\"\"\"\n",
    "    many = ['he',\"his\",\"him\",\"she\",'her',\"hers\",\n",
    "            'I','i','me','mine','my',\"i'm\",\"i've\",\n",
    "            'you','your','yours',\"you're\",\"you've\",\n",
    "            'it','its',\"it's\",\n",
    "            'they','their','theirs',\"they're\",\"they've\",\n",
    "            'we','us','our','ours',\"we're\",\"we've\"]\n",
    "    few = ['male','male','male','female','female','female',\n",
    "           'firstper','firstper','firstper','firstper','firstper','firstper','firstper',\n",
    "           'secper','secper','secper','secper','secper',\n",
    "           'neut3rd','neut3rd','neut3rd',\n",
    "           'neut3rd','neut3rd','neut3rd','neut3rd','neut3rd',\n",
    "          'coll3rd','coll3rd','coll3rd','coll3rd','coll3rd','coll3rd']\n",
    "    try:\n",
    "        idx = many.index(word)\n",
    "        word=few[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numwords():\n",
    "    #add numbers to common words\n",
    "    nums = np.arange(0,100)\n",
    "    ss =''\n",
    "    ss = ss.join([' '+str(n) for n in nums])\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ids_to_names():\n",
    "# Purpose of function is to exclude proper names of doctors - first and last names\n",
    "# Function will accept list of yelp_ids extracted by mysql from business table\n",
    "# Function will put together a list of names\n",
    "# Function will exclude terms like family doctor and pediatrics\n",
    "\n",
    "    dashed_names=[]#examples = ['tamalpais-pediatrics-novato','julie-doctor-kim']\n",
    "    sql = 'SELECT DISTINCT(yelp_id) FROM business;' \n",
    "    yelp_ids = query_SQL(sql)# extracts unique yelp_ids\n",
    "    for i in range(0,len(yelp_ids)):\n",
    "        dashed_names.append(yelp_ids[i][0])\n",
    "    \n",
    "    def split_append(yelp_id):\n",
    "        yelp_id = yelp_id.split('-')\n",
    "        #print(yelp_id)\n",
    "        return yelp_id\n",
    "\n",
    "#     business_type = ['pediatrics','family','medicine','doctor','institute','center',\n",
    "#                      'practice','md','san','francisco','ucsf','one','medical','group',\n",
    "#                     'sports','fitness','therapy','acupuncture','chiropractic','district']\n",
    "    \n",
    "    names = []\n",
    "#     for name in dashed_names:\n",
    "#         [names.append(l) for l in split_append(name) if l not in business_type]\n",
    "    for name in dashed_names:\n",
    "        [names.append(l) for l in split_append(name)]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pat_for_tokenizer():\n",
    "    from nltk.tokenize import regexp_tokenize\n",
    "    \"\"\"money and time patterns formatted to fit in nltk RegexpTokenizer object, debugged with s2 below\"\"\"\n",
    "    #s2 = 'it took until 12:30 am to get an appointment and I spent $3.50'\n",
    "    \n",
    "    pat = '\\w+|\\$[\\d\\.]+|\\S+'\n",
    "    pat2 = '\\d+\\:[\\d]+\\s?(pm|am)|\\S+'\n",
    "\n",
    "    pat3 = '('+pat2+')' +'|'+ '('+pat+')'#must be one string argument... | is OR\n",
    "    #tokenizer = RegexpTokenizer(pat3)#time\n",
    "    return pat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-92e374c54813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m texts = [[canonicalize(word) for word in tokenizer.tokenize(document.lower().replace('.',' ')) if word not in mystopwords]\n\u001b[0;32m---> 20\u001b[0;31m         for document in documents]#review in reviews \n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CLEAN & TOKENIZE - stoplists, lowercase, regular expressions, time-expressions\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#texts: A list of lists; text: list of tokenized words from 1 doc/review\n",
    "#document = a string\n",
    "\n",
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')#money\n",
    "#tokenizer = RegexpTokenizer('\\d*'+'\\:'+'\\d*')#time\n",
    "tokenizer = RegexpTokenizer(get_pat_for_tokenizer())#time & money\n",
    "useless_words = ['dr.','dr','doc','doctor','doctors',\"doctor's\",\"doctors'\",'get','see','could','would','have','had','did','yelp','try',\n",
    "                \"there's\",'ever',\"here's\",'here',\"couldn't\",'go','got','thing','things','made',\"don't\",'do','done','made','make',\n",
    "                \"it's\",'went','\\&','went']#type list\n",
    "mystopwords = stopwords.words() + ids_to_names() + useless_words\n",
    "#print(mystopwords[-500:])\n",
    "\n",
    "\n",
    "texts = [[canonicalize(word) for word in tokenizer.tokenize(document.lower().replace('.',' ')) if word not in mystopwords]\n",
    "        for document in documents]#review in reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# texts = [[canonicalize(word) for word in tokenizer.tokenize(document.lower()) if word not in stopwords.words()]\n",
    "#         for document in documents]#review in reviews \n",
    "\n",
    "print(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Context\n",
    "#result = [nltk.pos_tag(text) for text in texts[:2]]\n",
    "#print result\n",
    "\n",
    "simtext = nltk.Text(word for word in texts[1]) \n",
    "print(simtext.similar('help')) \n",
    "\n",
    "#print(simtext, np.shape(simtext))\n",
    "#print(simtext.similar('pain'))\n",
    "#simtext.similar('nasty')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parts of Speech\n",
    "result = [nltk.pos_tag(text) for text in texts[:2]]\n",
    "#nltk.help.upenn_tagset('RB')\n",
    "#print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(7608 unique tokens: [u'cussed', u'reconstruction,', u'yellow', u'four', u\"friend's\"]...)\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS\n",
    "dictionary = corpora.Dictionary(texts)# create dictionary object from cleaned texts\n",
    "dictionary.save('/Users/kaschbacher/insightproject/yelp/git-yelp/orthopedists.dict') # store the dictionary, for future reference\n",
    "print(dictionary)\n",
    "#print(type(dictionary))#<class 'gensim.corpora.dictionary.Dictionary'>\n",
    "#print(dictionary.token2id)# very long!\n",
    "\n",
    "# To merge with another dictionary -  Dictionary.merge_with())# Apply bag of words to reviews & print tokenized dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SERIALIZE & SAVE TO DISK \n",
    "\n",
    "# Convert tokenized documents to sparse vectors: -- [(0, 1), (4, 3),...\n",
    "# new_vec = dictionary.doc2bow(documents[1].lower().split())\n",
    "# print(new_vec)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus_mm = corpora.MmCorpus.serialize('orthopedists_bow.mm', corpus) # store to disk, for later use\n",
    "\n",
    "# corpus looks like - [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)... (35, 2)...\n",
    "# (word order index, frequency of occurence?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA  (takes awhile to run)\n",
    "# The constructor estimates Latent Dirichlet Allocation model parameters based on a training corpus \n",
    "# id2word is a mapping from word ids (integers) to words (strings). \n",
    "\n",
    "# 1) The constructor - obtains LDA model parameters given training corpus\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=100, update_every=0, passes=10, iterations=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE THE LDA OBJECT\n",
    "# --- note - i tried to do this with gensim, but it wasn't working, and ultimately cpickle did\n",
    "#from gensim.utils import SaveLoad\n",
    "#save(fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset([]), pickle_protocol=2)\n",
    "#utils.SaveLoad.save(lda, pickle_protocol=2)\n",
    "\n",
    "import cPickle as pickle\n",
    "pickle_out = open('lda_ortho_100t.p', 'wb')\n",
    "pickle.dump(lda, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PRINT THE TOPICS\n",
    "#lda.top_topics(corpus,num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# GAMMAS MATRIX: REVIEWS BY TOPICS, topic distributions over each review, given as proportions of all topics within a review\n",
    "gammas,_ = lda.inference(corpus)# first element returned by lda.inference is gammas matrix: #shape (3616,10)\n",
    "# Normalize:  sum over topics should equal one\n",
    "norm_gammas = gammas/(np.sum(gammas,axis=1)[:,None])# Axis=1 = sum over columns for each row; Axis=0 sum over rows in each column\n",
    "print(sum(norm_gammas[0][:]))#Yes, I checked that =1 - normalization worked\n",
    "norm_gammas_df = pd.DataFrame(norm_gammas)\n",
    "gammas_df = pd.DataFrame(gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01]\n"
     ]
    }
   ],
   "source": [
    "print(gammas[0][0:10])#row 0, all gammas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE ortho;\n",
      "Index([u'RID', u'BID',      1,      2,      3,      4,      5,      6,      7,\n",
      "            8, \n",
      "       ...\n",
      "           91,     92,     93,     94,     95,     96,     97,     98,     99,\n",
      "          100],\n",
      "      dtype='object', length=102)\n",
      "(13081, 102)\n"
     ]
    }
   ],
   "source": [
    "# Add the R-ID (primary key) and B-ID (foreign key) to the NORM-GAMMAS DF\n",
    "sql = 'SELECT id,business_id FROM review'+';'\n",
    "rows = ka.query_SQL(sql)# returns shape #reviews x 2\n",
    "\n",
    "# check that size are equivalent\n",
    "if np.shape(norm_gammas_df)[0] !=np.shape(rows)[0]:\n",
    "    print('Problem - rows in norm_gammas_df and sql query are not identical.')\n",
    "else:\n",
    "    #Convert sql return to array, and horizontally concatenate with gammas array, then convert to df\n",
    "    ids_array = np.array(rows)\n",
    "    \n",
    "    # Norm-gammas\n",
    "    ids_ngammas = np.hstack((ids_array, norm_gammas))#combine ids with norm gammas in numpy array\n",
    "    ids_ngammas_df = pd.DataFrame(ids_ngammas)\n",
    "    #---Rename columns (so topic # matches col) and confirm col-names, df.shape\n",
    "    ids_ngammas_df.columns = [np.arange(-1,ids_ngammas_df.shape[1]-1)]\n",
    "    ids_ngammas_df = ids_ngammas_df.rename(columns={-1: 'RID', 0: 'BID'})\n",
    "    \n",
    "    # Gammas\n",
    "    ids_gammas = np.hstack((ids_array, gammas))#combine ids with norm gammas in numpy array\n",
    "    ids_gammas_df = pd.DataFrame(ids_gammas)\n",
    "    #---Rename columns (so topic # matches col) and confirm col-names, df.shape\n",
    "    ids_gammas_df.columns = [np.arange(-1,ids_gammas_df.shape[1]-1)]\n",
    "    ids_gammas_df = ids_gammas_df.rename(columns={-1: 'RID', 0: 'BID'})\n",
    "    \n",
    "    print(ids_ngammas_df.columns)\n",
    "    print(ids_ngammas_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE THE NORM_GAMMAS DATAFRAME OBJECT\n",
    "\n",
    "import cPickle as pickle\n",
    "# pickle_out = open('norm_gammas_df.p', 'wb')\n",
    "# pickle.dump(norm_gammas_df, pickle_out)\n",
    "pickle_out = open('ids_ngammas_df.p', 'wb')\n",
    "pickle.dump(ids_ngammas_df, pickle_out)\n",
    "pickle_out = open('ids_gammas_df.p', 'wb')\n",
    "pickle.dump(ids_gammas_df, pickle_out)\n",
    "\n",
    "pickle_out = open('norm_gammas.p', 'wb')\n",
    "pickle.dump(norm_gammas, pickle_out)\n",
    "pickle_out = open('gammas.p', 'wb')\n",
    "pickle.dump(gammas, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.464545</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.464545</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.464545</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1     2     3     4     5     6     7     8     9     10   ...    91   \\\n",
       "BID                                                              ...          \n",
       "256  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  ...   0.01   \n",
       "274  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  ...   0.01   \n",
       "276  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  ...   0.01   \n",
       "294  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  ...   0.01   \n",
       "296  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  ...   0.01   \n",
       "\n",
       "      92    93    94    95    96        97    98    99    100  \n",
       "BID                                                            \n",
       "256  0.01  0.01  0.01  0.01  0.01  3.464545  0.01  0.01  0.01  \n",
       "274  0.01  0.01  0.01  0.01  0.01  0.010000  0.01  0.01  0.01  \n",
       "276  0.01  0.01  0.01  0.01  0.01  3.464545  0.01  0.01  0.01  \n",
       "294  0.01  0.01  0.01  0.01  0.01  0.010000  0.01  0.01  0.01  \n",
       "296  0.01  0.01  0.01  0.01  0.01  3.464545  0.01  0.01  0.01  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CALCULATE AVG topic distribution (gamma vector) for each BID (averaging over RID)\n",
    "# Should this be a new program, and just import ids_gammas_df?  Yes I think\n",
    "\n",
    "bid_gmeans = ids_gammas_df.drop('RID', 1)\n",
    "bid_gmeans = bid_gmeans.groupby('BID',axis=0,as_index='BID').mean()\n",
    "#bid_gmeans.columns = [np.arange(1,bid_gmeans.shape[1]+1)]# not needed, since I renamed columns of ids_gammas_df instead\n",
    "\n",
    "# Kept for reference syntax/comments\n",
    "###bid_gamma_means = bid_gamma_means.drop(bid_gamma_means.index[0])# No, this actually just deletes a real row of data, but it is the right syntax\n",
    "###bid_gamma_means.shape[0]#number of rows.  Matches: mysql query:  select count(distinct(business_id)) from review;\n",
    "\n",
    "# Print df to inspect\n",
    "#print(bid_gamma_means.loc[:,:])# works but prints everything\n",
    "#bid_gamma_means.head(5)\n",
    "bid_gmeans.tail(5)\n",
    "\n",
    "#bid_gmeans = bid_gmeans.sort([np.arange(1,(bid_gmeans.shape[1])+1)], ascending=False)#doesn't work\n",
    "bid_gmeans = bid_gmeans.sort([1,2,3,4,5],ascending=False)\n",
    "bid_gmeans.tail(5)\n",
    "\n",
    "# SAVE (ADDED to .GITIGNORE)\n",
    "# pickle_out = open('bid_gmeans.p', 'wb')\n",
    "# pickle.dump(bid_gmeans, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)#html object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save figure object. \n",
    "pickle.dump(vis_data, open('vis_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
