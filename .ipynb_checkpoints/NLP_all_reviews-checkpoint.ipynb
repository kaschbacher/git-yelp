{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint   # pretty-printer\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def call_mysql():\n",
    "    # pull in data from the reviews table\n",
    "    con = False\n",
    "    rows = np.nan\n",
    "    try:\n",
    "        con = pymysql.connect(host='localhost', port=3307, user='root', passwd='', db='yelpdata')\n",
    "        with con:\n",
    "            cur = con.cursor()\n",
    "            sql = 'USE yelpdata;'\n",
    "            cur.execute(sql)\n",
    "            sql = 'SELECT id,business_id,stars,comment FROM review;' \n",
    "            #print(sql)\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "#             for row in rows:\n",
    "#                 print(row)\n",
    "    except pymysql.Error, e:\n",
    "        print \"Error %d: %s\" % (e.args[0],e.args[1])\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "    return rows  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding some more information. Another great thing about him is that he was willing to experiment with me for my fibromyalgia. I brought in a book by a specialist and he was willing to try the protocol. Many doctors would not do that. He won't do anything crazy but will consider something reasonable. He also made himself available when I was having an emergency severe toothearache. He was at an offsite seminar, but his receptionist asked him to call me and he did, prescribed something and I think he even checked in with me later on or I called back. Either way, I got the treatment I needed.For those who think it takes a while to get ahold of him or his receptionistplease keep in mind it is one doctor and one receptionist. As you can see from all of the 5 star reviews, he spends time with his patients. Be a little patient yourself and you will be soon benefit from his great treatment.\n"
     ]
    }
   ],
   "source": [
    "rows = call_mysql()\n",
    "\n",
    "# UNDERSTAND THE OUTPUT OF THE MYSQL\n",
    "# print(np.shape(rows))#(3616, 4)\n",
    "# print(type(rows))#tuple\n",
    "# print(type(rows[:][0]))# still tuple\n",
    "# print(type(rows[3][3]))# int/str or whatever per the column\n",
    "\n",
    "#print(rows[0][3])#just the review for the zero-indexed row [0]\n",
    "# print(rows[1][3])#the review for the one-indexed row [1]\n",
    "\n",
    "# confusing - not printing all rows of the first column\n",
    "#print(rows[:][0])#all four columns of the data-row\n",
    "\n",
    "documents = []#type list\n",
    "ndocs = np.shape(rows)[0]\n",
    "\n",
    "#reviews = [[review for review in rows[3]] for row in rows]\n",
    "for row in rows:\n",
    "    documents.append(row[3])# row[3] is type string, documents is type list\n",
    "\n",
    "print(documents[0])# prints one review\n",
    "\n",
    "# for doc in range(0,ndocs):\n",
    "#     [review for review in rows[:][0]]# This prints the review from the id in the second []\n",
    "#     print review#type str\n",
    "\n",
    "# print(row[3])#just the review\n",
    "# print(row[:])# a row of data\n",
    "# print(review)# one review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def canonicalize(word):\n",
    "    #Assign all possible time values ('9:45','12:05',etc) to one word 'oclock'\n",
    "    match = re.search(r'\\d*'+':'+r'\\d*',word)\n",
    "    if match:\n",
    "        #print(match.group())\n",
    "        word = 'oclock'\n",
    "    \n",
    "    #Split words with a period in the middle\n",
    "    match = re.search(r'\\w*'+'.'+r'\\w*',word)\n",
    "    if match:\n",
    "        word = word.replace('.',' ')\n",
    "    \n",
    "    match = re.search(r'\\w*'+'emergency'+r'\\w*',word)\n",
    "    if match:\n",
    "        word = 'emergency'\n",
    "    \n",
    "    bads = ['dr.',\"n't\",\"'ve\",\"'re\",'wo',\"'m\"]\n",
    "    goods = ['dr','not','have','are','will','am']\n",
    "\n",
    "    try:\n",
    "        idx = bads.index(word)\n",
    "        word=goods[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word\n",
    "\n",
    "# This also works, better if you want to catch the error and do something, but I don't\n",
    "# try: \n",
    "#     match = re.search(r'\\d*'+':'+r'\\d*',s)\n",
    "#     test = match.group() \n",
    "# except AttributeError as e: \n",
    "#     print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pronouns(word):\n",
    "    many = ['he',\"his\",\"him\",\"she\",'her',\"hers\",'I','me','mine','my','you','your',\n",
    "            'yours','it','its','they','their','theirs','we','us','our','ours']\n",
    "    few = ['male','male','male','female','female','female','firstper','firstper',\n",
    "           'firstper','firstper','secper','secper','secper',\n",
    "           'neut3rdper','neut3rdper','neut3rdper','neut3rdper','neut3rdper',\n",
    "          'coll3rdper','coll3rdper','coll3rdper','coll3rdper']\n",
    "\n",
    "    try:\n",
    "        idx = many.index(word)\n",
    "        word=few[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK:  remove common words and tokenize\n",
    "common_words = 'for a as was \"was\" is of the and this that \"that\" sf to in if \",\" , ,, \\'s by on the to . .. ...  `` ? & * _ - / \"\\\" 3 2 5'\n",
    "# add numbers to common words\n",
    "range = np.arange(0,100)\n",
    "ss =''\n",
    "ss = ss.join([' '+str(i) for i in range])\n",
    "common_names = 'kong lisa chueng cheung dolev laroque caluag khoo usman kim jacoby mccaw simons wong eshima'\n",
    "common_words = common_words.join([' '+ss+' '+common_names])\n",
    "\n",
    "\n",
    "stoplist = set(common_words.split())\n",
    "\n",
    "#texts: A list of lists; text: list of tokenized words from 1 doc/review\n",
    "# texts = [[word for word in nltk.word_tokenize(document.lower()) if word not in stoplist]\n",
    "#         for document in documents]#review in reviews \n",
    "texts = [[pronouns(canonicalize(word)) for word in nltk.word_tokenize(document.lower()) if word not in stoplist]\n",
    "        for document in documents]#review in reviews \n",
    "\n",
    "#print(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20102 unique tokens: [u'risk  etc', u'200 00', u'radiologists', u'buccal', u'yellow']...)\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS\n",
    "dictionary = corpora.Dictionary(texts)# create dictionary object from cleaned texts\n",
    "#dictionary.save('/Users/kaschbacher/insightproject/yelp/git-yelp/doctor.dict') # store the dictionary, for future reference\n",
    "print(dictionary)\n",
    "#print(type(dictionary))#<class 'gensim.corpora.dictionary.Dictionary'>\n",
    "\n",
    "#print(dictionary.token2id)# very long!\n",
    "\n",
    "# To merge with another dictionary -  Dictionary.merge_with())# Apply bag of words to reviews & print tokenized dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SERIALIZE & SAVE TO DISK \n",
    "\n",
    "# Convert tokenized documents to sparse vectors: -- [(0, 1), (4, 3),...\n",
    "# new_vec = dictionary.doc2bow(documents[1].lower().split())\n",
    "# print(new_vec)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus_mm = corpora.MmCorpus.serialize('/tmp/doctor.mm', corpus) # store to disk, for later use\n",
    "\n",
    "#print(corpus)\n",
    "# looks like - [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)... (35, 2)...\n",
    "# word order index -frequency of occurence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tf-idf model\n",
    "# term frequency - inverse document frequency\n",
    "# higher index - occured a lot in a few docs:  log(D/d) ---  uniqueness/singularity\n",
    "# D = # of doc, d = # of doc that that word appears in\n",
    "# tf-idf will convert any vector from bag-of-words integer counts to Tfidf real-valued weights\n",
    "\n",
    "# CAUTION - The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and/or runtime exceptions.\n",
    "\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "\n",
    "#  Apply tfidf to the whole corpus\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "# for doc in corpus_tfidf:\n",
    "#     print(doc)\n",
    "\n",
    "\n",
    "# example transformation\n",
    "# doc_bow = [(0,1),(1,3)]\n",
    "# print(tfidf[doc_bow]); print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.lsimodel.LsiModel'>\n"
     ]
    }
   ],
   "source": [
    "# LSI MODEL\n",
    "# Transformations can also be serialized, one on top of another, in a sort of chain:\n",
    "# LSI is another transformation, which acts on the Tf-idf-weighted space\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6, power_iters=10)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "lsi.print_topics(6)\n",
    "row = [[d[1] for d in doc] for doc in corpus_lsi]\n",
    "print(type(lsi))\n",
    "lsi.save('/Users/kaschbacher/insightproject/yelp/git-yelp/lsi.dict')\n",
    "\n",
    "# sort documents on topic assignments\n",
    "# numpy isn't the right structure for sorting based on the first column - dataframe\n",
    "# array = np.asarray(row).reshape(5,3)\n",
    "row = [[d[1] for d in doc] for doc in corpus_lsi]\n",
    "df = pd.DataFrame(row).sort(0,axis=0,ascending=False)#0 sorts on the first column\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'0.244*\"female\" + 0.203*\"male\" + 0.136*\"secondperson\" + 0.123*\"was\" + 0.121*\"!\" + 0.110*\"collect3rdperson\" + 0.110*\"neut3rdperson\" + 0.105*\"that\" + 0.097*\",\" + 0.096*\"firstperson\"',\n",
       " u'-0.879*\"female\" + 0.285*\"male\" + 0.065*\"surgery\" + 0.048*\"neut3rdperson\" + 0.044*\"was\" + -0.040*\"milkman\" + -0.038*\"bedside\" + 0.037*\"pain\" + 0.037*\"that\" + -0.036*\"cares\"',\n",
       " u'0.707*\"collect3rdperson\" + 0.201*\"male\" + 0.127*\"son\" + -0.125*\"surgery\" + 0.105*\"daughter\" + -0.094*\"female\" + -0.090*\"was\" + 0.085*\"caring\" + 0.084*\"has\" + 0.081*\"pediatrician\"',\n",
       " u'0.472*\"collect3rdperson\" + -0.252*\"male\" + -0.163*\"!\" + -0.154*\"best\" + -0.136*\"very\" + -0.133*\"highly\" + -0.127*\"recommend\" + -0.113*\"great\" + 0.109*\"$\" + -0.100*\"caring\"',\n",
       " u'0.416*\"surgery\" + 0.204*\"collect3rdperson\" + -0.193*\"secondperson\" + 0.146*\"surgeon\" + 0.139*\"knee\" + 0.131*\"pain\" + -0.124*\"doctor\" + 0.118*\"procedure\" + 0.114*\"was\" + -0.109*\"medical\"',\n",
       " u'-0.259*\"secondperson\" + 0.145*\"knee\" + 0.139*\"appointment\" + 0.139*\"chen\" + 0.133*\"was\" + -0.129*\"look\" + -0.125*\"mosser\" + 0.116*\"thorough\" + 0.109*\"called\" + 0.106*\"highly\"']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(lsi.print_topics()))#(5,)\n",
    "#lsi.print_topics()\n",
    "# print('a topic: ',lsi.print_topics()[0][:20])#first n chars of string\n",
    "\n",
    "#lsi.print_topics()\n",
    "#print(lsi.print_debug())# None??\n",
    "# lsi.show_topics()\n",
    "\n",
    "\n",
    "# retrain model with more data\n",
    "# sort documents based on topic assignments\n",
    "# e.g., look at the top five documents assigned to this topic to get a sense what it's about\n",
    "# you train model on many doctors' reviews, but when you do the prediction, you can project\n",
    "# back onto a single doctor\n",
    "# each review might contain a weightings of topics - 70%(T1)+30%(T2)\n",
    "# give each new doctor a star rating per topic - that's the readout - personalized report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA  (takes awhile to run)\n",
    "# The constructor estimates Latent Dirichlet Allocation model parameters based on a training corpus \n",
    "# id2word is a mapping from word ids (integers) to words (strings). \n",
    "\n",
    "# I think you can't give it the tfidf_corpus - seems not to work/not sure why though\n",
    "# lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5, alpha=auto)\n",
    "# doc_lda = lda[corpus_tfidf]\n",
    "\n",
    "# 1) The constructor - obtains LDA model parameters given training corpus\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=10, update_every=0, passes=10, iterations=100)\n",
    "\n",
    "# 2) Infer topic distributions on new, unseen documents:\n",
    "#doc_lda = lda[mmcorpus]\n",
    "doc_lda = lda[corpus]\n",
    "\n",
    "# 3) Update model with new documents:\n",
    "#lda.update(new_training_corpus)\n",
    "#lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0.044426429282918886, u'female'), (0.024594040190564603, u'and'), (0.020746306400343757, u','), (0.019574843276996073, u' '), (0.014199064001544255, u'i')], -0.86004217862364207)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([(0.044426429282918886, u'female'),\n",
       "   (0.024594040190564603, u'and'),\n",
       "   (0.020746306400343757, u','),\n",
       "   (0.019574843276996073, u' '),\n",
       "   (0.014199064001544255, u'i')],\n",
       "  -0.86004217862364207),\n",
       " ([(0.045786505507638152, u','),\n",
       "   (0.037335262883056261, u' '),\n",
       "   (0.030974352889445844, u'and'),\n",
       "   (0.0302371329613676, u'to'),\n",
       "   (0.028789394479197681, u'the')],\n",
       "  -0.97312414397536162),\n",
       " ([(0.035792451040030329, u','),\n",
       "   (0.029614682929964782, u'to'),\n",
       "   (0.028963535614022932, u'the'),\n",
       "   (0.025669236589629635, u'i'),\n",
       "   (0.025408723582341104, u' ')],\n",
       "  -1.1007921390186388),\n",
       " ([(0.050442486506677556, u'i'),\n",
       "   (0.045048420342704669, u' '),\n",
       "   (0.036975271847549764, u'and'),\n",
       "   (0.036736057798340091, u'firstper'),\n",
       "   (0.028654083352738086, u'to')],\n",
       "  -1.1478667613391202),\n",
       " ([(0.047161136866373292, u' '),\n",
       "   (0.035680371493364352, u'i'),\n",
       "   (0.0289893511445613, u','),\n",
       "   (0.025369317657303905, u'the'),\n",
       "   (0.023350364611845566, u'and')],\n",
       "  -1.2215366066574709),\n",
       " ([(0.041427453442641052, u' '),\n",
       "   (0.038296469608143549, u'the'),\n",
       "   (0.037555657742544095, u'and'),\n",
       "   (0.030951394541283716, u','),\n",
       "   (0.025656947513843939, u'to')],\n",
       "  -1.3040357498504904),\n",
       " ([(0.042838280861677819, u' '),\n",
       "   (0.039000858161529435, u'i'),\n",
       "   (0.030510018267688149, u','),\n",
       "   (0.029928681545038426, u'the'),\n",
       "   (0.029231489613553104, u'to')],\n",
       "  -1.4302094020628167),\n",
       " ([(0.050463284071437041, u'male'),\n",
       "   (0.047098870237483881, u' '),\n",
       "   (0.038860032839396839, u','),\n",
       "   (0.035478869302774822, u'and'),\n",
       "   (0.027373473699718293, u'is')],\n",
       "  -1.5724658050888718),\n",
       " ([(0.028558884912954885, u'female'),\n",
       "   (0.012009048798519608, u'laurence'),\n",
       "   (0.0096374812821976383, u' '),\n",
       "   (0.0093332690227097177, u'is'),\n",
       "   (0.0085024859804024213, u'the')],\n",
       "  -20.40690836352184),\n",
       " ([(0.027580626226759577, u'female'),\n",
       "   (0.0045473280834493097, u' '),\n",
       "   (0.0035639937670937181, u'beltran'),\n",
       "   (0.0030177743121141149, u'is'),\n",
       "   (0.0028645280822894616, u'and')],\n",
       "  -20.852261342346512)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda.top_topics(corpus, num_words=5)[:][0])\n",
    "lda.top_topics(corpus, num_words=5)#shape (10,2) for num_words=5\n",
    "#lda.print_topics(10)\n",
    "\n",
    "#row = [[d[1] for d in doc] for doc in doc_lda]\n",
    "#df = pd.DataFrame(row).sort(0,axis=0,ascending=False)#0 sorts on the first column\n",
    "\n",
    "# Old comment from gensim_tut1\n",
    "# As expected, the first five documents are more strongly related to the second topic \n",
    "# while the remaining four documents to the first topic:\n",
    "\n",
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "# for doc in corpus_lsi: \n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.data import find\n",
    "# from nltk.parse import bllip\n",
    "# #from bllipparser import RerankingParser, tokenize\n",
    "\n",
    "# sentence = 'I really liked this doctor. We really got along well.  She was really easy to talk to.'\n",
    "# new = nltk.word_tokenize(sentence)\n",
    "# print(new)\n",
    "\n",
    "# #nltk.parse.bllip.BllipParser()\n",
    "# model_dir = find('models/bllip_wsj_no_aux').path\n",
    "# #bllip = bllip.BllipParser.from_unified_model_dir(model_dir)\n",
    "# top_parse = bllip.parse_one(sentence)#not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "Dictionary(102 unique tokens: [u'all', u'reviewers', u'bedside', u'thumbs', u'tamalpais']...)\n",
      "<gensim.interfaces.TransformedCorpus object at 0x10d3ccf10>\n",
      "<gensim.interfaces.TransformedCorpus object at 0x1126aa2d0>\n"
     ]
    }
   ],
   "source": [
    "# Create a new corpus & apply LDA model to the new corpus\n",
    "# https://radimrehurek.com/gensim/models/ldamodel.html#id2\n",
    "# https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation\n",
    "\n",
    "new_docs = [u\"\"\"We love Tamalpais Pediatrics and our children's doctor, John Lee. Like other reviewers have found, with their doctors, \n",
    "           Dr. Lee is patient, listens and addresses concerns we have about our childrens health.  I don't know if all pediatricians will come to the hospital to meet the new baby, but Dr. Lee has come to see both our daughters after they were born...\n",
    "           We've seen Dr. Branco when Dr. Lee was unavailable and were relieved to see he and Dr. Lee are very similar in bedside manners.  \n",
    "        Crystal Cox, the PA, is also a dream.  Shes warm and very easygoing with our young children.  I've no problems having her see the kids when one of the MD s are not available.\n",
    "        I'm sorry to see Deana S. did not have a good experience with the receptionist, but I have to say that I've never had any problems with any of the staff.  They are all professional and very caring with the children.  \n",
    "        5 Stars!  Thumbs up!\"\"\"]\n",
    "# QUICK FORMAT: remove extra spaces and new lines\n",
    "new_doc = [\" \".join(element.replace(\"\\n\",\"\").split()) for element in new_doc]\n",
    "print(np.shape(new_doc))\n",
    "#print(new_doc)\n",
    "\n",
    "# I'll want to make this a function that I call later on\n",
    "# NLTK code\n",
    "# remove common words and tokenize\n",
    "common_words2 = 'for a of the and to in is \\'s by on the to . `` ? _ - / \"\\\"'\n",
    "stoplist2 = set(common_words2.split())\n",
    "\n",
    "#texts: A list of lists; text: list of tokenized words from 1 doc/review\n",
    "texts2 = [[word2 for word2 in nltk.word_tokenize(doc.lower()) if word2 not in stoplist2]\n",
    "        for doc in new_docs]#review in reviews \n",
    "#print(texts2)\n",
    "\n",
    "# bag of words\n",
    "new_dictionary = corpora.Dictionary(texts2)# create dictionary object from cleaned texts\n",
    "new_dictionary.save('/tmp/tamalpais_pediatrics.dict') # store the dictionary, for future reference\n",
    "print(new_dictionary)\n",
    "#print(new_dictionary.token2id)\n",
    "\n",
    "# form new corpus\n",
    "new_corpus = [new_dictionary.doc2bow(text) for text in texts2]\n",
    "# A trained model can used be to transform new, unseen documents \n",
    "# (plain bag-of-words count vectors) into LDA topic distributions:\n",
    "\n",
    "# get topic probability distribution for a new review: TransformedCorpus' object\n",
    "new_doc_lda = lda[new_corpus]\n",
    "#new_doc_lda.print_topics()#doesn't work\n",
    "print(new_doc_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
