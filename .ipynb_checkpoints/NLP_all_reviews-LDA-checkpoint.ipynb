{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gensim\n",
    "from gensim import corpora, models, similarities, utils\n",
    "from pprint import pprint   # pretty-printer\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymysql\n",
    "import re\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def call_mysql():\n",
    "    # pull in data from the reviews table\n",
    "    con = False\n",
    "    rows = np.nan\n",
    "    try:\n",
    "        con = pymysql.connect(host='localhost', port=3307, user='root', passwd='', db='yelpdata')\n",
    "        with con:\n",
    "            cur = con.cursor()\n",
    "            sql = 'USE yelpdata;'\n",
    "            cur.execute(sql)\n",
    "            sql = 'SELECT id,business_id,stars,comment FROM review;' \n",
    "            #print(sql)\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "#             for row in rows:\n",
    "#                 print(row)\n",
    "    except pymysql.Error, e:\n",
    "        print \"Error %d: %s\" % (e.args[0],e.args[1])\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "    return rows  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding some more information. Another great thing about him is that he was willing to experiment with me for my fibromyalgia. I brought in a book by a specialist and he was willing to try the protocol. Many doctors would not do that. He won't do anything crazy but will consider something reasonable. He also made himself available when I was having an emergency severe toothearache. He was at an offsite seminar, but his receptionist asked him to call me and he did, prescribed something and I think he even checked in with me later on or I called back. Either way, I got the treatment I needed.For those who think it takes a while to get ahold of him or his receptionistplease keep in mind it is one doctor and one receptionist. As you can see from all of the 5 star reviews, he spends time with his patients. Be a little patient yourself and you will be soon benefit from his great treatment.\n"
     ]
    }
   ],
   "source": [
    "rows = call_mysql()\n",
    "\n",
    "# UNDERSTAND THE OUTPUT OF THE MYSQL\n",
    "# print(np.shape(rows))#(3616, 4)\n",
    "# print(type(rows))#tuple\n",
    "# print(type(rows[:][0]))# still tuple\n",
    "# print(type(rows[3][3]))# int/str or whatever per the column\n",
    "\n",
    "#print(rows[0][3])#just the review for the zero-indexed row [0]\n",
    "# print(rows[1][3])#the review for the one-indexed row [1]\n",
    "\n",
    "# confusing - not printing all rows of the first column\n",
    "#print(rows[:][0])#all four columns of the data-row\n",
    "\n",
    "documents = []#type list\n",
    "ndocs = np.shape(rows)[0]\n",
    "\n",
    "#reviews = [[review for review in rows[3]] for row in rows]\n",
    "for row in rows:\n",
    "    documents.append(row[3])# row[3] is type string, documents is type list\n",
    "\n",
    "print(documents[0])# prints one review\n",
    "\n",
    "# for doc in range(0,ndocs):\n",
    "#     [review for review in rows[:][0]]# This prints the review from the id in the second []\n",
    "#     print review#type str\n",
    "\n",
    "# print(row[3])#just the review\n",
    "# print(row[:])# a row of data\n",
    "# print(review)# one review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unify_time(word):\n",
    "    #Assign all possible time values ('9:45','12:05',etc) to one word 'oclock'\n",
    "    match = re.search(r'\\d*'+':'+r'\\d*',word)\n",
    "    if match:\n",
    "        #print(match.group())\n",
    "        word = 'oclock'\n",
    "    return word\n",
    "\n",
    "\n",
    "    # This also works, better if you want to catch the error and do something, but I don't\n",
    "    # try: \n",
    "    #     match = re.search(r'\\d*'+':'+r'\\d*',s, word)\n",
    "    #     test = match.group() \n",
    "    #     word = 'oclock'\n",
    "    # except AttributeError as e: \n",
    "    #     print('error in unify_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def period_split(word):\n",
    "    # Split words with a period in the middle - not yet the ideal implementation b/c it returns two words as one\n",
    "    match = re.search(r'\\w*'+'.'+r'\\w*',word)\n",
    "    if match:\n",
    "        word = word.replace('.',' ')\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def canonicalize(word):    \n",
    "    bads = ['dr.',\"n't\",\"'ve\",\"'re\",'wo',\"'m\"]\n",
    "    goods = ['dr','not','have','are','will','am']\n",
    "\n",
    "    try:\n",
    "        idx = bads.index(word)\n",
    "        word=goods[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unify_pronouns(word):\n",
    "    \"\"\" map many onto one for pronouns - parts of speech\"\"\"\n",
    "    many = ['he',\"his\",\"him\",\"she\",'her',\"hers\",'I','i','me','mine','my','you','your',\n",
    "            'yours','it','its','they','their','theirs','we','us','our','ours']\n",
    "    few = ['male','male','male','female','female','female','firstper','firstper',\n",
    "           'firstper','firstper','firstper','secper','secper','secper',\n",
    "           'neut3rd','neut3rd','neut3rd','neut3rd','neut3rd',\n",
    "          'coll3rd','coll3rd','coll3rd','coll3rd']\n",
    "    try:\n",
    "        idx = many.index(word)\n",
    "        word=few[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(word):\n",
    "    # wrapper function - performs a bunch of functions on a given word\n",
    "    word = unify_time(word)\n",
    "    word = unify_pronouns(word)\n",
    "    word = canonicalize(word)\n",
    "    word = period_split(word)    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numwords():\n",
    "    #add numbers to common words\n",
    "    nums = np.arange(0,100)\n",
    "    ss =''\n",
    "    ss = ss.join([' '+str(n) for n in nums])\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_SQL():\n",
    "    con = False\n",
    "    rows = []\n",
    "    try:\n",
    "        con = pymysql.connect(host='localhost', port=3307, user='root', passwd='', db='yelpdata')\n",
    "        with con:\n",
    "            cur = con.cursor()\n",
    "            sql = 'USE yelpdata;'\n",
    "            cur.execute(sql)\n",
    "            sql = 'SELECT DISTINCT(yelp_id) FROM business;' \n",
    "            #print(sql)\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "            #print('business id from table: ',rows, type(rows))\n",
    "    except pymysql.Error, e:\n",
    "        print \"Error %d: %s\" % (e.args[0],e.args[1])\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "    return rows  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ids_to_names():\n",
    "# Purpose of function is to exclude proper names of doctors - first and last names\n",
    "# Function will accept list of yelp_ids extracted by mysql from business table\n",
    "# Function will put together a list of names\n",
    "# Function will exclude terms like family doctor and pediatrics\n",
    "\n",
    "    dashed_names=[]#examples = ['tamalpais-pediatrics-novato','julie-doctor-kim']\n",
    "    yelp_ids = query_SQL()# extracts unique yelp_ids\n",
    "    for i in range(0,len(yelp_ids)):\n",
    "        dashed_names.append(yelp_ids[i][0])\n",
    "    \n",
    "    def split_append(yelp_id):\n",
    "        yelp_id = yelp_id.split('-')\n",
    "        #print(yelp_id)\n",
    "        return yelp_id\n",
    "\n",
    "    business_type = ['pediatrics','family','medicine','doctor','institute','center',\n",
    "                     'practice','md','san','francisco','ucsf','one','medical','group',\n",
    "                    'sports','fitness','therapy','acupuncture','chiropractic','district']\n",
    "    names = []\n",
    "    for name in dashed_names:\n",
    "        [names.append(l) for l in split_append(name) if l not in business_type]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['-', '24', '25', '26', '27', '20', '21', '22', '23', '28', '29', '0', '4', '8', '...', 'to', '59', '58', '55', '54', '57', '56', '51', '50', '53', '52', '19', '88', '89', '82', '83', '80', '81', '86', '87', '84', '85', 'for', '/', '3', '7', '?', 'by', '_', 'on', '39', '38', '33', '32', '31', '30', '37', '36', '35', '34', ',', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '..', '&', '*', '.', '2', '6', 'was', '``', 'that', '99', '98', '91', '90', '93', '92', '95', '94', '97', '96', '11', '10', '13', '12', '15', '14', '17', '16', 'this', '18', 'of', 'and', 'is', 'as', 'in', 'if', '48', '49', '46', '47', ',,', '45', '42', '43', '40', '41', '1', '5', '9', \"'s\", '\"\"', '77', '76', '75', '74', '73', '72', '71', '70', '79', '78', 'a', 'sf', 'the', '44'])\n"
     ]
    }
   ],
   "source": [
    "# I may end up deleting this.  Now I used regular expressions.  But ids_to_names is not used\n",
    "\n",
    "# NLTK:  remove common words and tokenize\n",
    "\n",
    "# Explore this alternative\n",
    "#from nltk.corpus import stopwords\n",
    "# for w in stopwords.words():\n",
    "#     print(w)\n",
    "\n",
    "common_words = 'for a as was is of the and this that sf to in if , ,, \\'s by on the to . .. ...  `` ? & * _ - / \"\\\" 3 2 5'\n",
    "nums = numwords()\n",
    "common_words = common_words+nums\n",
    "\n",
    "#common_names = 'kong lisa chueng cheung dolev laroque caluag khoo usman kim jacoby mccaw simons wong eshima'\n",
    "common_names = ids_to_names()\n",
    "#common_words = common_words.join([' '+nums+' '+common_names])\n",
    "#print(common_words)\n",
    "\n",
    "stoplist = set(common_words.split())\n",
    "print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pat_for_tokenizer():\n",
    "    from nltk.tokenize import regexp_tokenize\n",
    "    \"\"\"money and time patterns formatted to fit in nltk RegexpTokenizer object, debugged with s2 below\"\"\"\n",
    "    #s2 = 'it took until 12:30 am to get an appointment and I spent $3.50'\n",
    "    \n",
    "    pat = '\\w+|\\$[\\d\\.]+|\\S+'\n",
    "    pat2 = '\\d+\\:[\\d]+\\s?(pm|am)|\\S+'\n",
    "\n",
    "    pat3 = '('+pat2+')' +'|'+ '('+pat+')'#must be one string argument... | is OR\n",
    "    #tokenizer = RegexpTokenizer(pat3)#time\n",
    "    return pat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['adding', 'information.', 'another', 'great', 'thing', 'willing', 'experiment', 'fibromyalgia.', 'brought', 'book', 'specialist', 'willing', 'try', 'protocol.', 'many', 'doctors', 'would', 'that.', \"won't\", 'anything', 'crazy', 'consider', 'something', 'reasonable.', 'made', 'available', 'emergency', 'severe', 'toothearache.', 'offsite', 'seminar,', 'receptionist', 'asked', 'call', 'did,', 'prescribed', 'something', 'think', 'even', 'checked', 'later', 'called', 'back.', 'either', 'way,', 'got', 'treatment', 'needed.for', 'think', 'takes', 'get', 'ahold', 'receptionistplease', 'keep', 'mind', 'one', 'doctor', 'one', 'receptionist.', 'see', '5', 'star', 'reviews,', 'spends', 'time', 'patients.', 'little', 'patient', 'soon', 'benefit', 'great', 'treatment.'], [\"it's\", 'tough', 'write', 'review,', \"i've\", 'patient', 'dr', 'least', '3', 'years', 'now,', 'more.', \"i'm\", 'huge', 'fan', 'personally,', \"he's\", 'always', 'given', 'sound', 'advice', 'easy', 'work', 'with,', 'recently.', \"he's\", 'lost', 'personal', 'touch.the', 'result', 'one', 'best', 'rated', 'doctors', 'yelp', 'sf', 'goes', 'massive', 'population', 'increase', 'supply', 'longer', 'meet', 'demand.', \"it's\", 'become', 'incredibly', 'tough', 'get', 'appointment', 'see', 'get', 'real', 'oneonone', 'care.', 'people', 'complained,', 'understand', \"he's\", 'taking', 'new', 'patients', 'unless', \"it's\", 'serious', 'illness', 'given', 'barely', 'time', 'see', 'existing', 'patients.', 'song', 'dance', 'years', 'ago', 'get', 'see', 'him.', 'point,', 'think', 'needs', 'stop', 'taking', 'new', 'patients', 'entirely.first', 'frustration', 'round:', 'went', 'physical', 'may,', 'ordered', 'bunch', 'labs', 'check', 'blood,', 'cholesterol,', 'etc.', 'except', 'failed', 'tell', '2', 'labs', 'go', 'left', 'appointment', 'assistant', 'never', 'followed', 'give', 'information.', 'busy', 'travel', 'overseas', 'work,', 'forgot', 'feel', 'like', 'one', 'call', 'inquire', 'it.second', 'frustration', 'round:', 'since', 'may,', 'past', '3', 'times', \"i've\", 'called', 'appointment,', \"i've\", 'unable', 'get', 'see', 'week', \"i've\", 'gone', 'instead', 'urgent', 'care', 'centers', '2', '3', 'times', 'one', 'food', 'poisoning,', 'another', 'fairly', 'severe', 'viral', 'infection', \"couldn't\", 'wait', '5', 'days', 'see', 'him.third', 'frustration', 'round:', '3rd', 'time,', 'called', 'make', 'appointment', '5', 'weeks', 'persistent', 'heartburn,', 'made', 'exception', 'complained', 'unable', 'get', 'appointment', 'assistant.', 'took', '2', 'days,', 'even', 'still,', 'call', 'back', 'tell', 'would', 'see', 'hours', '4:45pm', 'hours,', 'open', '9am,', 'clue', 'doctor', 'hours', 'never', 'cease', 'amaze', 'me.', 'told', 'might', 'bacterial,', 'never', 'heartburn', 'historically,', 'test', 'would', 'tell', 'whether', 'medication', 'assessment', 'specialist.', 'ordered', 'another', 'set', 'labs,', 'including', 'ones', \"weren't\", 'originally', 'processed', 'physical', 'went', 'quest.', 'fourth', 'frustration', 'round:', 'test', 'results', 'delivered', 'assistant', 'phone.', 'him,', 'knew', 'nothing', 'commentary', 'regarding', 'meds', 'specialist.', 'simple,', 'came', 'back', 'negative...with', 'solution.', 'asked', \"wasn't\", 'able', 'pick', 'phone', 'call', 'speak', 'directly,', 'since', \"he's\", 'person', 'familiar', 'case.', 'said', \"she'd\", 'send', 'message', 'let', 'know.', 'called', 'morning', 'check', 'back', 'instead', 'calling', 'me,', 'like', 'asked,', 'read', 'verbatim', 'message', \"he'd\", 'typed', 'telling', 'take', 'labs', 'go', 'see', 'conlin,', 'gastronomist.', 'asked', \"couldn't\", 'call', 'directly', 'discuss', 'this,', 'said', \"he's\", 'busy', 'make', 'calls', 'patients', 'would', 'leaving', '3', 'weeks', 'vacation', 'compounded', 'issue.why', 'time,', 'dr', 'dan,', 'text', 'assistant', 'lengthy', 'message,', \"can't\", 'simply', 'pick', 'phone', 'dial', 'directly?', 'and,', \"can't\", 'call', 'patients', 'directly', 'busy,', 'even', 'bothering', 'take', 'new', 'clients', \"still?i'm\", 'going', 'one', 'medical', 'on.', \"you've\", 'lost', 'patient.']]\n"
     ]
    }
   ],
   "source": [
    "# Two ways to clean & tokenize - stoplists, lowercase, regular expressions, time-expressions\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#texts: A list of lists; text: list of tokenized words from 1 doc/review\n",
    "#document = a string\n",
    "\n",
    "# 1) uses my stoplist - see above\n",
    "# texts = [[clean(word) for word in nltk.word_tokenize(document.lower()) if word not in stoplist]\n",
    "#         for document in documents]#review in reviews \n",
    "\n",
    "#tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')#money\n",
    "#tokenizer = RegexpTokenizer('\\d*'+'\\:'+'\\d*')#time\n",
    "tokenizer = RegexpTokenizer(get_pat_for_tokenizer())#time & money\n",
    "\n",
    "# 2) uses nltk functions\n",
    "texts = [[canonicalize(word) for word in tokenizer.tokenize(document.lower()) if word not in stopwords.words()]\n",
    "        for document in documents]#review in reviews \n",
    "\n",
    "print(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(31837 unique tokens: [u'considered,', u'hanging', u'ovaries,', u'aileen', u'ovaries.']...)\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS\n",
    "dictionary = corpora.Dictionary(texts)# create dictionary object from cleaned texts\n",
    "#dictionary.save('/Users/kaschbacher/insightproject/yelp/git-yelp/doctor.dict') # store the dictionary, for future reference\n",
    "print(dictionary)\n",
    "#print(type(dictionary))#<class 'gensim.corpora.dictionary.Dictionary'>\n",
    "#print(dictionary.token2id)# very long!\n",
    "\n",
    "# To merge with another dictionary -  Dictionary.merge_with())# Apply bag of words to reviews & print tokenized dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SERIALIZE & SAVE TO DISK \n",
    "\n",
    "# Convert tokenized documents to sparse vectors: -- [(0, 1), (4, 3),...\n",
    "# new_vec = dictionary.doc2bow(documents[1].lower().split())\n",
    "# print(new_vec)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpus_mm = corpora.MmCorpus.serialize('doctor_bow_for_lda.mm', corpus) # store to disk, for later use\n",
    "\n",
    "#print(corpus)\n",
    "# looks like - [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)... (35, 2)...\n",
    "# (word order index, frequency of occurence?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrain model with more data\n",
    "# sort documents based on topic assignments\n",
    "# e.g., look at the top five documents assigned to this topic to get a sense what it's about\n",
    "# you train model on many doctors' reviews, but when you do the prediction, you can project\n",
    "# back onto a single doctor\n",
    "# each review might contain a weightings of topics - 70%(T1)+30%(T2)\n",
    "# product: personalized report of topics in that Drs reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA  (takes awhile to run)\n",
    "# The constructor estimates Latent Dirichlet Allocation model parameters based on a training corpus \n",
    "# id2word is a mapping from word ids (integers) to words (strings). \n",
    "\n",
    "# 1) The constructor - obtains LDA model parameters given training corpus\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=10, update_every=0, passes=10, iterations=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SAVE THE OBJECT\n",
    "# --- note - i tried to do this with gensim, but it wasn't working, and ultimately cpickle did\n",
    "#from gensim.utils import SaveLoad\n",
    "#save(fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset([]), pickle_protocol=2)\n",
    "#utils.SaveLoad.save(lda, pickle_protocol=2)\n",
    "\n",
    "import cPickle as pickle\n",
    "pickle_out = open('lda.p', 'wb')\n",
    "pickle.dump(lda, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-31-aee696393350>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-aee696393350>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pickle.dump(vis_data, open('vis_data.p','wb')\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(vis_data, open('vis_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
