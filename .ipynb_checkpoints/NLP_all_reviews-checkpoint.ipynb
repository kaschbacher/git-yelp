{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint   # pretty-printer\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymysql\n",
    "import re\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def call_mysql():\n",
    "    # pull in data from the reviews table\n",
    "    con = False\n",
    "    rows = np.nan\n",
    "    try:\n",
    "        con = pymysql.connect(host='localhost', port=3307, user='root', passwd='', db='yelpdata')\n",
    "        with con:\n",
    "            cur = con.cursor()\n",
    "            sql = 'USE yelpdata;'\n",
    "            cur.execute(sql)\n",
    "            sql = 'SELECT id,business_id,stars,comment FROM review;' \n",
    "            #print(sql)\n",
    "            cur.execute(sql)\n",
    "            rows = cur.fetchall()\n",
    "#             for row in rows:\n",
    "#                 print(row)\n",
    "    except pymysql.Error, e:\n",
    "        print \"Error %d: %s\" % (e.args[0],e.args[1])\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "    return rows  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding some more information. Another great thing about him is that he was willing to experiment with me for my fibromyalgia. I brought in a book by a specialist and he was willing to try the protocol. Many doctors would not do that. He won't do anything crazy but will consider something reasonable. He also made himself available when I was having an emergency severe toothearache. He was at an offsite seminar, but his receptionist asked him to call me and he did, prescribed something and I think he even checked in with me later on or I called back. Either way, I got the treatment I needed.For those who think it takes a while to get ahold of him or his receptionistplease keep in mind it is one doctor and one receptionist. As you can see from all of the 5 star reviews, he spends time with his patients. Be a little patient yourself and you will be soon benefit from his great treatment.\n"
     ]
    }
   ],
   "source": [
    "rows = call_mysql()\n",
    "\n",
    "# UNDERSTAND THE OUTPUT OF THE MYSQL\n",
    "# print(np.shape(rows))#(3616, 4)\n",
    "# print(type(rows))#tuple\n",
    "# print(type(rows[:][0]))# still tuple\n",
    "# print(type(rows[3][3]))# int/str or whatever per the column\n",
    "\n",
    "#print(rows[0][3])#just the review for the zero-indexed row [0]\n",
    "# print(rows[1][3])#the review for the one-indexed row [1]\n",
    "\n",
    "# confusing - not printing all rows of the first column\n",
    "#print(rows[:][0])#all four columns of the data-row\n",
    "\n",
    "documents = []#type list\n",
    "ndocs = np.shape(rows)[0]\n",
    "\n",
    "#reviews = [[review for review in rows[3]] for row in rows]\n",
    "for row in rows:\n",
    "    documents.append(row[3])# row[3] is type string, documents is type list\n",
    "\n",
    "print(documents[0])# prints one review\n",
    "\n",
    "# for doc in range(0,ndocs):\n",
    "#     [review for review in rows[:][0]]# This prints the review from the id in the second []\n",
    "#     print review#type str\n",
    "\n",
    "# print(row[3])#just the review\n",
    "# print(row[:])# a row of data\n",
    "# print(review)# one review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def canonicalize(word):\n",
    "    #Assign all possible time values ('9:45','12:05',etc) to one word 'oclock'\n",
    "    match = re.search(r'\\d*'+':'+r'\\d*',word)\n",
    "    if match:\n",
    "        #print(match.group())\n",
    "        word = 'oclock'\n",
    "    \n",
    "    #Split words with a period in the middle\n",
    "    match = re.search(r'\\w*'+'.'+r'\\w*',word)\n",
    "    if match:\n",
    "        word = word.replace('.',' ')\n",
    "    \n",
    "    match = re.search(r'\\w*'+'emergency'+r'\\w*',word)\n",
    "    if match:\n",
    "        word = 'emergency'\n",
    "    \n",
    "    bads = ['dr.',\"n't\",\"'ve\",\"'re\",'wo',\"'m\"]\n",
    "    goods = ['dr','not','have','are','will','am']\n",
    "\n",
    "    try:\n",
    "        idx = bads.index(word)\n",
    "        word=goods[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word\n",
    "\n",
    "# This also works, better if you want to catch the error and do something, but I don't\n",
    "# try: \n",
    "#     match = re.search(r'\\d*'+':'+r'\\d*',s)\n",
    "#     test = match.group() \n",
    "# except AttributeError as e: \n",
    "#     print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pronouns(word):\n",
    "    many = ['he',\"his\",\"him\",\"she\",'her',\"hers\",'I','me','mine','my','you','your',\n",
    "            'yours','it','its','they','their','theirs','we','us','our','ours']\n",
    "    few = ['male','male','male','female','female','female','firstperson','firstperson',\n",
    "           'firstperson','firstperson','secondperson','secondperson','secondperson',\n",
    "           'neut3rdperson','neut3rdperson','neut3rdperson','neut3rdperson','neut3rdperson',\n",
    "          'collect3rdperson','collect3rdperson','collect3rdperson','collect3rdperson']\n",
    "\n",
    "    try:\n",
    "        idx = many.index(word)\n",
    "        word=few[idx]\n",
    "    except ValueError as e:\n",
    "        word=word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK:  remove common words and tokenize\n",
    "common_words = 'for a as was \"was\" \",\" \"that\" is of the and this that was sf to in if , \\'s by on the to . ...  `` ? _ - / \"\\\" 3 2 5'\n",
    "# add numbers to common words\n",
    "range = np.arange(0,100)\n",
    "ss =''\n",
    "ss = ss.join([' '+str(i) for i in range])\n",
    "common_names = 'kong lisa chueng cheung dolev laroque caluag khoo usman kim jacoby mccaw simons wong eshima'\n",
    "common_words = common_words.join([' '+ss+' '+common_names])\n",
    "\n",
    "\n",
    "stoplist = set(common_words.split())\n",
    "\n",
    "#texts: A list of lists; text: list of tokenized words from 1 doc/review\n",
    "# texts = [[word for word in nltk.word_tokenize(document.lower()) if word not in stoplist]\n",
    "#         for document in documents]#review in reviews \n",
    "texts = [[pronouns(canonicalize(word)) for word in nltk.word_tokenize(document.lower()) if word not in stoplist]\n",
    "        for document in documents]#review in reviews \n",
    "\n",
    "#print(texts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20102 unique tokens: [u'risk  etc', u'200 00', u'radiologists', u'buccal', u'yellow']...)\n"
     ]
    }
   ],
   "source": [
    "# BAG OF WORDS\n",
    "dictionary = corpora.Dictionary(texts)# create dictionary object from cleaned texts\n",
    "dictionary.save('/tmp/doctor.dict') # store the dictionary, for future reference\n",
    "print(dictionary)\n",
    "#print(type(dictionary))#<class 'gensim.corpora.dictionary.Dictionary'>\n",
    "\n",
    "#print(dictionary.token2id)\n",
    "\n",
    "# To merge with another dictionary -  Dictionary.merge_with())# Apply bag of words to reviews & print tokenized dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SERIALIZE & SAVE TO DISK \n",
    "\n",
    "# Convert tokenized documents to sparse vectors: -- [(0, 1), (4, 3),...\n",
    "# new_vec = dictionary.doc2bow(documents[1].lower().split())\n",
    "# print(new_vec)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('/tmp/doctor.mm', corpus) # store to disk, for later use\n",
    "\n",
    "#print(corpus)\n",
    "# looks like - [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)... (35, 2)...\n",
    "# word order index -frequency of occurence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tf-idf model\n",
    "# term frequency - inverse document frequency\n",
    "# higher index - occured a lot in a few docs:  log(D/d) ---  uniqueness/singularity\n",
    "# D = # of doc, d = # of doc that that word appears in\n",
    "# tf-idf will convert any vector from bag-of-words integer counts to Tfidf real-valued weights\n",
    "\n",
    "# CAUTION - The same vector space (= the same set of feature ids) must be used for training as well as for subsequent vector transformations. Failure to use the same input feature space, such as applying a different string preprocessing, using different feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will result in feature mismatch during transformation calls and consequently in either garbage output and/or runtime exceptions.\n",
    "\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "\n",
    "#  Apply tfidf to the whole corpus\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "# for doc in corpus_tfidf:\n",
    "#     print(doc)\n",
    "\n",
    "\n",
    "# example transformation\n",
    "# doc_bow = [(0,1),(1,3)]\n",
    "# print(tfidf[doc_bow]); print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transformations can also be serialized, one on top of another, in a sort of chain:\n",
    "# LSI is another transformation, which acts on the Tf-idf-weighted space\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=6)\n",
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "lsi.print_topics(6)\n",
    "row = [[d[1] for d in doc] for doc in corpus_lsi]\n",
    "\n",
    "# lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5, alpha=\"auto\")\n",
    "# doc_lda = lda[corpus_tfidf]\n",
    "# lda.print_topics(5)\n",
    "# row = [[d[1] for d in doc] for doc in doc_lda]\n",
    "\n",
    "# Old comment from gensim_tut1\n",
    "# As expected, the first five documents are more strongly related to the second topic \n",
    "# while the remaining four documents to the first topic:\n",
    "\n",
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "# for doc in corpus_lsi: \n",
    "#     print(doc)\n",
    "\n",
    "# sort documents on topic assignments\n",
    "# numpy isn't the right structure for sorting based on the first column - dataframe\n",
    "#row = [[d[1] for d in doc] for doc in corpus_lsi]\n",
    "\n",
    "# array = np.asarray(row).reshape(5,3)\n",
    "df = pd.DataFrame(row).sort(0,axis=0,ascending=False)#0 sorts on the first column\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not unicode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-131738da99de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(5,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mlsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# retrain model with more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# sort documents based on topic assignments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# e.g., look at the top five documents assigned to this topic to get a sense what it's about\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not unicode"
     ]
    }
   ],
   "source": [
    "print(np.shape(lsi.print_topics()))#(5,)\n",
    "lsi.print_topics()\n",
    "# retrain model with more data\n",
    "# sort documents based on topic assignments\n",
    "# e.g., look at the top five documents assigned to this topic to get a sense what it's about\n",
    "# you train model on many doctors' reviews, but when you do the prediction, you can project\n",
    "# back onto a single doctor\n",
    "# each review might contain a weightings of topics - 70%(T1)+30%(T2)\n",
    "# give each new doctor a star rating per topic - that's the readout - personalized report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
